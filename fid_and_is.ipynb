{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[constructor]  ==== flash_if_available=True (0/20), fused_if_available=True (fusing_add_ln=0/20, fusing_mlp=0/20) ==== \n",
      "    [VAR config ] embed_dim=1280, num_heads=20, depth=20, mlp_ratio=4.0\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0833333 (tensor([0.0000, 0.0044, 0.0088, 0.0132, 0.0175, 0.0219, 0.0263, 0.0307, 0.0351,\n",
      "        0.0395, 0.0439, 0.0482, 0.0526, 0.0570, 0.0614, 0.0658, 0.0702, 0.0746,\n",
      "        0.0789, 0.0833]))\n",
      "\n",
      "[init_weights] VAR with init_std=0.0161374\n",
      "prepare finished.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 20    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}\n",
    "\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt = 'vae_ch160v4096z32.pth'\n",
    "var_ckpt = 'local_output/ar-ckpt-last.pth'\n",
    "# var_ckpt = 'local_output/tb-VARd20__pn1_2_3_4_5_6_8_10_13_16__b24ep100adamlr0.0001wd0.05/events.out.tfevents.1717576276.volcan.3422325.0__0605_1631'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt):\n",
    "    print('path does not exist!')\n",
    "\n",
    "# build vae, var\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'vae' not in globals() or 'var' not in globals():\n",
    "    vae, var = build_vae_var(\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "        device=device, patch_nums=patch_nums, depth=MODEL_DEPTH, shared_aln=False,\n",
    "    )\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "var.load_state_dict(torch.load(var_ckpt, map_location='cpu')['trainer']['var_wo_ddp'], strict=True)\n",
    "vae.eval(), var.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "for p in var.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# 2. generate val images & resize real val images\n",
    "import clip\n",
    "import dist\n",
    "import pickle\n",
    "from PIL import Image\n",
    "# set args\n",
    "seed = 0 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "num_sampling_steps = 250 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "cfg = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "more_smooth = False # True for more smooth output\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run faster\n",
    "tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = bool(tf32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = bool(tf32)\n",
    "torch.set_float32_matmul_precision('high' if tf32 else 'highest')\n",
    "\n",
    "generated_folder_path = 'generated_resize'\n",
    "real_resize_folder_path = 'val_resize'\n",
    "if not os.path.exists(generated_folder_path):\n",
    "    os.makedirs(generated_folder_path)\n",
    "if not os.path.exists(real_resize_folder_path):\n",
    "    os.makedirs(real_resize_folder_path)\n",
    "val_text_feature = np.load('dataset/COCO2017/annotation_text_features_val.npy')\n",
    "val_anno_path = 'dataset/COCO2017/annotations/val_images_anno.pkl'\n",
    "with open(val_anno_path, 'rb') as file:\n",
    "    val_anno_data = pickle.load(file)\n",
    "for i in range(val_text_feature.shape[0]):\n",
    "    text_features_BD = torch.tensor(val_text_feature[i]).expand(1,-1).to(device)\n",
    "    real_img_path = val_anno_data[i][0]\n",
    "    old_path = '/root/data_new/zejia/workspace/psl/var/datasets'\n",
    "    new_path = 'dataset'\n",
    "    real_img_path = real_img_path.replace(old_path,new_path)\n",
    "    img_num = real_img_path.split('/')[-1]\n",
    "    B = 1\n",
    "    if os.path.exists(os.path.join(generated_folder_path, img_num)):\n",
    "        continue\n",
    "\n",
    "    real_image = Image.open(real_img_path)\n",
    "    real_resize_img = real_image.resize((299,299),PImage.LANCZOS)\n",
    "    real_save_path = os.path.join(real_resize_folder_path, img_num)\n",
    "    real_image.save(real_save_path)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast('cuda', enabled=True, dtype=torch.float16, cache_enabled=True):    # using bfloat16 can be faster\n",
    "            # recon_B3HW = var.autoregressive_infer_cfg(B=B, label_B=label_B, cfg=cfg, top_k=900, top_p=0.95, g_seed=seed, more_smooth=more_smooth)\n",
    "            recon_B3HW = var.autoregressive_infer_cfg(B=B, text_features_BD=text_features_BD)\n",
    "    chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "    chw = chw.clone().permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "    chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "    chw = chw.resize((299,299),PImage.LANCZOS)\n",
    "    generated_save_path = os.path.join(generated_folder_path, img_num)\n",
    "    chw.save(generated_save_path)\n",
    "\n",
    "print(f'Images saved to {generated_folder_path, real_save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate IS\n",
    "from datasets import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "from scipy.stats import entropy\n",
    "from torchvision.models.inception import inception_v3\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ISImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(root) + \"/*.jpg\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)]).convert('RGB')      \n",
    "        item_image = self.transform(img)\n",
    "        return item_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "path = 'generated_resize'\n",
    "count = 0\n",
    "for root,dirs,files in os.walk(path):   \n",
    "      for each in files:\n",
    "             count += 1  \n",
    "print(count)\n",
    "batch_size = 64\n",
    "transforms_ = [\n",
    "    transforms.Resize((256, 256), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "]\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    ISImageDataset(path, transforms_=transforms_),\n",
    "    batch_size = batch_size,\n",
    ")\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print('cuda: ',cuda)\n",
    "tensor = torch.cuda.FloatTensor\n",
    "\n",
    "inception_model = inception_v3(pretrained=True, transform_input=False).cuda()\n",
    "inception_model.eval()\n",
    "up = nn.Upsample(size=(299, 299), mode='bilinear', align_corners=False).cuda()\n",
    "\n",
    "def get_pred(x):\n",
    "    if True:\n",
    "        x = up(x)\n",
    "    x = inception_model(x)\n",
    "    return F.softmax(x, dim=1).data.cpu().numpy()\n",
    "\n",
    "print('Computing predictions using inception v3 model')\n",
    "preds = np.zeros((count, 1000))\n",
    "\n",
    "for i, data in enumerate(val_dataloader):\n",
    "    data = data.type(tensor)\n",
    "    batch_size_i = data.size()[0]\n",
    "    preds[i * batch_size:i * batch_size + batch_size_i] = get_pred(data)\n",
    "\n",
    "print('Computing KL Divergence')\n",
    "split_scores = []\n",
    "splits=10\n",
    "N = count\n",
    "for k in range(splits):\n",
    "    part = preds[k * (N // splits): (k + 1) * (N // splits), :] # split the whole data into several parts\n",
    "    py = np.mean(part, axis=0)  # marginal probability\n",
    "    scores = []\n",
    "    for i in range(part.shape[0]):\n",
    "        pyx = part[i, :]  # conditional probability\n",
    "        scores.append(entropy(pyx, py))  # compute divergence\n",
    "    split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "\n",
    "mean, std  = np.mean(split_scores), np.std(split_scores)\n",
    "print('IS is %.4f' % mean)\n",
    "print('The std is %.4f' % std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
